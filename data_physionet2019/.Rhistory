xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(SepsisLabel))
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(data$SepsisLabel))
library(tree)
library(ISLR)
library(randomForest)
library(e1071)
library(Matrix)
library(xgboost)
files <- list.files("~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/trainingA_exp/", pattern=".psv", ignore.case=T)
library("data.table")
ff <- function(input){
data <- fread(input)
}
a <- lapply(files, ff)
library(plyr)
data <- ldply(a, function(x) rbind(x, fill = TRUE))
data <- subset(data, select = -c(8,37, 38))
# test<-read.csv(file="~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/training/p000300.psv",sep = "|") #load test data
for (i in 1:ncol(data)){
data[is.na(data[,i]),i]<- median(data[,i],na.rm =TRUE)} #用中位数替代缺失值
data$SepsisLabel <- as.factor(data$SepsisLabel)#转换数据类型为因子
###XGBoost
#Label conversion
species = data$SepsisLabel
label = as.integer(data$SepsisLabel)#convert SepsisLabel to label
data$SepsisLabel = NULL
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(data[train.index,])
train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(data$SepsisLabel))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
# Review the final model and results
xgb.fit
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(data$SepsisLabel))
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(species))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
# Review the final model and results
xgb.fit
library(tree)
library(ISLR)
library(randomForest)
library(e1071)
library(Matrix)
library(xgboost)
files <- list.files("~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/trainingA_exp/", pattern=".psv", ignore.case=T)
library("data.table")
ff <- function(input){
data <- fread(input)
}
a <- lapply(files, ff)
library(plyr)
data <- ldply(a, function(x) rbind(x, fill = TRUE))
data <- subset(data, select = -c(8,37, 38))
# test<-read.csv(file="~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/training/p000300.psv",sep = "|") #load test data
for (i in 1:ncol(data)){
data[is.na(data[,i]),i]<- median(data[,i],na.rm =TRUE)} #用中位数替代缺失值
###XGBoost
#Label conversion
species = data$SepsisLabel
label = as.integer(data$SepsisLabel)#convert SepsisLabel to label
data$SepsisLabel = NULL
#Split the data for training and testing (75/25 split)
n = nrow(data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(data[train.index,])
train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(species))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(species))
###XGBoost
#Label conversion
species = data$SepsisLabel
label = as.integer(data$SepsisLabel)#convert SepsisLabel to label
data$SepsisLabel = NULL
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(species))
data$SepsisLabel <- as.factor(data$SepsisLabel)#转换数据类型为因子
library(tree)
library(ISLR)
library(randomForest)
library(e1071)
library(Matrix)
library(xgboost)
files <- list.files("~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/trainingA_exp/", pattern=".psv", ignore.case=T)
library("data.table")
ff <- function(input){
data <- fread(input)
}
a <- lapply(files, ff)
library(plyr)
data <- ldply(a, function(x) rbind(x, fill = TRUE))
data <- subset(data, select = -c(8,37, 38))
# test<-read.csv(file="~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/training/p000300.psv",sep = "|") #load test data
for (i in 1:ncol(data)){
data[is.na(data[,i]),i]<- median(data[,i],na.rm =TRUE)} #用中位数替代缺失值
data$SepsisLabel <- as.factor(data$SepsisLabel)#转换数据类型为因子
###XGBoost
#Label conversion
species = data$SepsisLabel
label = as.integer(data$SepsisLabel)#convert SepsisLabel to label
data$SepsisLabel = NULL
#Split the data for training and testing (75/25 split)
n = nrow(data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(data[train.index,])
train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(levels(species))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(as.numeric(levels(species)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
View(label)
maximum
Max(lable)
Maximum(lable)
max(label)
min(label)
label = as.integer(data$SepsisLabel) - 1#convert SepsisLabel to label
data$SepsisLabel = NULL
#Split the data for training and testing (75/25 split)
n = nrow(data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(data[train.index,])
train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(as.numeric(levels(species)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
min(label)
library(tree)
library(ISLR)
library(randomForest)
library(e1071)
library(Matrix)
library(xgboost)
files <- list.files("~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/trainingA_exp/", pattern=".psv", ignore.case=T)
library("data.table")
ff <- function(input){
data <- fread(input)
}
a <- lapply(files, ff)
library(plyr)
data <- ldply(a, function(x) rbind(x, fill = TRUE))
data <- subset(data, select = -c(8,37, 38))
# test<-read.csv(file="~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/training/p000300.psv",sep = "|") #load test data
for (i in 1:ncol(data)){
data[is.na(data[,i]),i]<- median(data[,i],na.rm =TRUE)} #用中位数替代缺失值
data$SepsisLabel <- as.factor(data$SepsisLabel)#转换数据类型为因子
###XGBoost
#Label conversion
species = data$SepsisLabel
label = as.integer(data$SepsisLabel) -1#convert SepsisLabel to label
data$SepsisLabel = NULL
min(label)
#Split the data for training and testing (75/25 split)
n = nrow(data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(data[train.index,])
train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(as.numeric(levels(species)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
# Review the final model and results
xgb.fit
# Review the final model and results
xgb.fit
# Predict outcomes with the test data
xgb.pred = predict(xgb.fit,test.data,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(species)
#Identify the class with the highest probability for each prediction
# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(species)[test.label+1]
# Calculate the final accuracy
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
View(xgb.pred)
table <- table(xgb.pred$prediction, xgb.pred$label) #参数分别是预测的labels,原本的labels
View(table)
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table)) #准确率
accuracy <- (table[1,1]+table[2,2])/sum(table) #准确率
table <- table(xgb.pred$prediction, xgb.pred$label)
# Calculate the final accuracy
#result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
accuracy <- (table[1,1]+table[2,2])/sum(table) #准确率
precision <- (table[2,2])/(table[2,2]+table[2,1]) #精确率
recall <- (table[2,2])/(table[2,2]+table[1,2]) #覆盖率
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*accuracy)))
print(paste("Final Precision =",sprintf("%1.2f%%", 100*precision)))
print(paste("Final Recall =",sprintf("%1.2f%%", 100*recall)))
View(table)
library(pROC)
install.packages("pROC")
library(pROC)
roc(xgb.pred$label, xgb.pred$prediction, plot=TRUE)
View(xgb.pred$prediction)
roc(xgb.pred$label, xgb.pred$prediction[,1], plot=TRUE)
roc(xgb.pred$label, xgb.pred$prediction[1,], plot=TRUE)
roc(xgb.pred$label, xgb.pred$prediction, plot=TRUE)
plot.roc(xgb.pred$label, xgb.pred$prediction, print.auc=TRUE, print.auc.y=TRUE)
View(xgb.pred)
plot.roc(xgb.pred$label, xgb.pred, print.auc=TRUE, print.auc.y=TRUE)
roc(xgb.pred$label, xgb.pred, algorithm = 2)
# Predict outcomes with the test data
xgb.pred = predict(xgb.fit,test.data,reshape=T, probability =TRUE)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(species)
#Identify the class with the highest probability for each prediction
# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(species)[test.label+1]
table <- table(xgb.pred$prediction, xgb.pred$label)
# Calculate the final accuracy
#result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
accuracy <- (table[1,1]+table[2,2])/sum(table) #准确率
precision <- (table[2,2])/(table[2,2]+table[2,1]) #精确率
recall <- (table[2,2])/(table[2,2]+table[1,2]) #覆盖率
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*accuracy)))
print(paste("Final Precision =",sprintf("%1.2f%%", 100*precision)))
print(paste("Final Recall =",sprintf("%1.2f%%", 100*recall)))
library(pROC)
roc(xgb.pred$label, xgb.pred, algorithm = 2)
library(tree)
library(ISLR)
library(randomForest)
library(e1071)
library(Matrix)
library(xgboost)
files <- list.files("~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/trainingA_exp/", pattern=".psv", ignore.case=T)
library("data.table")
ff <- function(input){
data <- fread(input)
}
a <- lapply(files, ff)
library(plyr)
data <- ldply(a, function(x) rbind(x, fill = TRUE))
data <- subset(data, select = -c(8,37, 38))
# test<-read.csv(file="~/Desktop/KI2/2.Current research trend/5.Block 2/4.PhysioNet challenge/training/p000300.psv",sep = "|") #load test data
for (i in 1:ncol(data)){
data[is.na(data[,i]),i]<- median(data[,i],na.rm =TRUE)} #用中位数替代缺失值
data$SepsisLabel <- as.factor(data$SepsisLabel)#转换数据类型为因子
###XGBoost
#Label conversion
species = data$SepsisLabel
label = as.integer(data$SepsisLabel) -1#convert SepsisLabel to label
data$SepsisLabel = NULL
#Split the data for training and testing (75/25 split)
n = nrow(data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(data[train.index,])
train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
num_class = length(as.numeric(levels(species)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
# Review the final model and results
xgb.fit
# Predict outcomes with the test data
xgb.pred = predict(xgb.fit,test.data,reshape=T, probability =TRUE)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(species)
#Identify the class with the highest probability for each prediction
# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(species)[test.label+1]
roc(xgb.pred$label, xgb.pred, algorithm = 2)
