train.label = label[train.index]
test.data = as.matrix(data[-train.index,])
test.label = label[-train.index]
#Create the xgb.DMatrix objects
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
#Define the main parameters
# Define the parameters for multinomial classification
#num_class = length(as.numeric(levels(species)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=5
)
#Train the model
# Train the XGBoost classifer
xgb.fit=xgb.train(
params=params,
data=xgb.train,
nrounds=10000,
nthreads=1,
early_stopping_rounds=10,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
View(label)
# Predict outcomes with the test data
xgb.pred = predict(xgb.fit,test.data,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(species)
#Identify the class with the highest probability for each prediction
# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(species)[test.label+1]
table <- table(xgb.pred$prediction, xgb.pred$label)
# Calculate the final accuracy
accuracy <- (table[1,1]+table[2,2])/sum(table)
precision <- (table[2,2])/(table[2,2]+table[2,1])
recall <- (table[2,2])/(table[2,2]+table[1,2])
print(paste("Final Accuracy of XGBoost =",sprintf("%1.2f%%", 100*accuracy)))
print(paste("Final Precision of XGBoost =",sprintf("%1.2f%%", 100*precision)))
print(paste("Final Recall of XGBoost =",sprintf("%1.2f%%", 100*recall)))
########SVM
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[,-1]
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE) #用cut()函数设置10个folds
# store our metrics for each model
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
svm.model <- svm(data_all.train$SepsisLabel~ ., data =
data_all.train, kernel = "polynomial")
prediction <- predict(svm.model, data_all.test)
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of SVM =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of SVM =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of SVM =",sprintf("%1.2f%%", 100*recallaverage)))
View(data)
View(data_all.train)
########Random Forest
# store our metrics for each model
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
View(table)
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
########Random Forest
# store our metrics for each model
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
View(data)
View(data)
View(data)
data <- data[1:200,-1]
View(data)
data$SepsisLabel <- as.factor(data$SepsisLabel)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
View(data)
########Random Forest
# store our metrics for each model
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:200,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
View(data_all.train)
library(tree)
library(ISLR)
library(randomForest)
library(e1071)
library(Matrix)
library(xgboost)
library("data.table")
########Random Forest
# store our metrics for each model
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:200,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
View(data)
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:200,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
accuracy <- vector()
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:200,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:500,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
########Random Forest
# store our metrics for each model
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:1000,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
View(rf.data)
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
data <- data[,-1]
########Random Forest
# store our metrics for each model
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
rf.data <- randomForest(data_all.train$SepsisLabel~ ., data =
data_all.train,ntree=10)
prediction <- predict(rf.data, data_all.test, type="class")
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of RF =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of RF =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of RF =",sprintf("%1.2f%%", 100*recallaverage)))
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[1:1000,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
# store our metrics for each model
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
svm.model <- svm(data_all.train$SepsisLabel~ ., data =
data_all.train, kernel = "radial")
prediction <- predict(svm.model, data_all.test)
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of SVM =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of SVM =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of SVM =",sprintf("%1.2f%%", 100*recallaverage)))
########SVM
data <-read.csv(file="/Users/leeo/Desktop/KI2/2.Current\ research\ trend/5.Block\ 2/6.github/interpretability-project/processed_data/proceseed_data.csv",sep = ",")
data <- data[,-1]
data$SepsisLabel <- as.factor(data$SepsisLabel)
set.seed(3)
datarandom<-data[sample(nrow(data)),] #shuffle the data
folds <- cut(seq(1,nrow(data)),breaks=10,labels=FALSE)
# store our metrics for each model
accuracy <- vector()
precision <- vector()
recall <- vector()
#start for loop
for (i in 1:10){
testIndexes <- which(folds==i,arr.ind=TRUE)
trainIndexes <- which(folds!=i,arr.ind=TRUE)
data_all.test <- datarandom[testIndexes, ]
data_all.train <- datarandom[trainIndexes, ]
#Use the test and train data partitions
set.seed(3)
svm.model <- svm(data_all.train$SepsisLabel~ ., data =
data_all.train, kernel = "radial")
prediction <- predict(svm.model, data_all.test)
#generate the confusion matrix
table <- table(prediction, data_all.test$SepsisLabel)
#end for loop
accuracy <- c(accuracy,(table[1,1]+table[2,2])/sum(table))
precision <- c(precision,(table[2,2])/(table[2,2]+table[2,1]))
recall <- c(recall,(table[2,2])/(table[2,2]+table[1,2]))
}
table
accuracyaverage = mean(accuracy)
precisionaverage = mean(precision)
recallaverage = mean(recall)
print(paste("Final Accuracy of SVM =",sprintf("%1.2f%%", 100*accuracyaverage)))
print(paste("Final Precision of SVM =",sprintf("%1.2f%%", 100*precisionaverage)))
print(paste("Final Recall of SVM =",sprintf("%1.2f%%", 100*recallaverage)))
